{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d272ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import ConvModule, constant_init, kaiming_init\n",
    "from mmcv.runner import load_checkpoint\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from mmcls.models.backbones.base_backbone import BaseBackbone\n",
    "import torch\n",
    "from mmcls.models.builder import BACKBONES\n",
    "import math\n",
    "from mmcls.models.builder import build_classifier,build_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a82ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class SGBlock(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, keep_3x3=False):\n",
    "        super(SGBlock, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = inp // expand_ratio\n",
    "        if hidden_dim < oup / 6.:\n",
    "            hidden_dim = math.ceil(oup / 6.)\n",
    "            hidden_dim = _make_divisible(hidden_dim, 16)# + 16\n",
    "\n",
    "        #self.relu = nn.ReLU6(inplace=True)\n",
    "        self.identity = False\n",
    "        self.identity_div = 1\n",
    "        self.expand_ratio = expand_ratio\n",
    "        if expand_ratio == 2:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(inp, inp, 3, 1, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(oup, oup, 3, stride, 1, groups=oup, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        elif inp != oup and stride == 1 and keep_3x3 == False:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw-linear\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU6(inplace=True),\n",
    "            )\n",
    "        elif inp != oup and stride == 2 and keep_3x3==False:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw-linear\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(oup, oup, 3, stride, 1, groups=oup, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            if keep_3x3 == False:\n",
    "                self.identity = True\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(inp, inp, 3, 1, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                #nn.ReLU6(inplace=True),\n",
    "                # pw\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(oup, oup, 3, 1, 1, groups=oup, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        \n",
    "        \n",
    "        if self.identity:\n",
    "            shape = x.shape\n",
    "            chock = (shape[1]//self.identity_div,out.shape[1]-shape[1]//self.identity_div)\n",
    "            out =torch.split(out,chock,dim=1)\n",
    "            p_out=out[0]+x\n",
    "            #id_tensor = x[:,:shape[1]//self.identity_div,:,:]\n",
    "            # id_tensor = torch.cat([x[:,:shape[1]//self.identity_div,:,:],torch.zeros(shape)[:,shape[1]//self.identity_div:,:,:].cuda()],dim=1)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            #out[:,:shape[1]//self.identity_div,:,:] = out[:,:shape[1]//self.identity_div,:,:] + id_tensor\n",
    "            return torch.cat([p_out,out[1]],dim=1)\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "@BACKBONES.register_module()\n",
    "class MXNet(BaseBackbone):\n",
    "    def __init__(self, num_classes=1000, width_mult=1.):\n",
    "        super(MXNet, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = [\n",
    "            # t, c, n, s\n",
    "            [2,  96, 1, 2],\n",
    "            [6, 144, 1, 1],\n",
    "            [6, 192, 3, 2],\n",
    "            [6, 288, 3, 2],\n",
    "            [6, 384, 4, 1],\n",
    "            [6, 576, 4, 2],\n",
    "            [6, 960, 3, 1],\n",
    "            [6,1280, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(32 * width_mult, 4 if width_mult == 0.1 else 8)\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = SGBlock\n",
    "        for t, c, n, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 4 if width_mult == 0.1 else 8)\n",
    "            if c == 1280 and width_mult < 1:\n",
    "                output_channel = 1280\n",
    "            layers.append(block(input_channel, output_channel, s, t, n==1 and s==1))\n",
    "            input_channel = output_channel\n",
    "            for i in range(n-1):\n",
    "                layers.append(block(input_channel, output_channel, 1, t))\n",
    "                input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        input_channel = output_channel\n",
    "        output_channel = _make_divisible(input_channel, 4) # if width_mult == 0.1 else 8) if width_mult > 1.0 else input_channel\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(output_channel, num_classes)\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        if isinstance(pretrained, str):\n",
    "            logger = logging.getLogger()\n",
    "            load_checkpoint(self, pretrained, strict=False, logger=logger)\n",
    "        elif pretrained is None:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    kaiming_init(m)\n",
    "                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n",
    "                    constant_init(m, 1)\n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and self.norm_eval:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, _BatchNorm):\n",
    "                    m.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36271f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MXNet()\n",
    "test.eval()\n",
    "x =torch.randn(1,3,224,224)\n",
    "#torch.onnx.export(test,x,'test.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "41040248",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg= {'type':'MXNet'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c00d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = build_backbone(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9798949",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MXNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd794e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
