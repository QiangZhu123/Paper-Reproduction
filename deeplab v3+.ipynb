{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99df38d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from mmcv.cnn.bricks.conv_module import ConvModule\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aff08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparaConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels):\n",
    "        super().__init__()\n",
    "        self.depth = nn.Conv2d(in_channels,\n",
    "                               in_channels,\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               stride=1,\n",
    "                               groups=in_channels)\n",
    "        self.point =nn.Conv2d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size=1,\n",
    "                              padding=0,\n",
    "                              stride=1)\n",
    "    def forward(self,x):\n",
    "        out =self.depth(x)\n",
    "        out = self.point(out)\n",
    "        return out\n",
    "    \n",
    "class SeparaBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                out_channels,\n",
    "                downsample=True,\n",
    "                first_layer=False):\n",
    "        super().__init__()\n",
    "        if first_layer:\n",
    "            self.layer1=SeparaConv(in_channels,out_channels)\n",
    "        else:\n",
    "            self.layer1=nn.Sequential(nn.ReLU(),\n",
    "                                    SeparaConv(in_channels,out_channels))\n",
    "        \n",
    "        self.layer2=nn.Sequential(nn.ReLU(),\n",
    "                                SeparaConv(out_channels,out_channels))\n",
    "        self.size = nn.ModuleList()\n",
    "        if downsample:\n",
    "            self.layer3 = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "            self.size.append(nn.Conv2d(in_channels,out_channels,kernel_size=1,padding=0,stride=2))\n",
    "        else:\n",
    "            self.layer3= nn.Sequential(nn.ReLU(),\n",
    "                                SeparaConv(out_channels,out_channels))\n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        out =self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out =self.layer3(out)\n",
    "        for layer in self.size:\n",
    "            residual  =layer(x)\n",
    "        return out+residual\n",
    "class Entry(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "                nn.Conv2d(3,32,kernel_size=3,stride=2,padding=1),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Conv2d(32,64,kernel_size=3),\n",
    "                nn.ReLU()\n",
    "                                 )\n",
    "        self.conv = nn.Conv2d(3,32,kernel_size=3,stride=2,padding=1)\n",
    "        self.block1 = SeparaBlock(64,128,first_layer=True)\n",
    "        self.block2 = SeparaBlock(128,256)\n",
    "        self.block3 =SeparaBlock(256,728)\n",
    "    def forward(self,x):\n",
    "        #out = self.conv(x)\n",
    "        out =self.stem(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        return out\n",
    "class MiddleBlock(nn.Module):\n",
    "    def __init__(self,in_channels,nums):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(nums):\n",
    "            self.blocks.append(SeparaBlock(728,728,downsample=False))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x\n",
    "        for layer in self.blocks:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "class ExitBlock(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.block1 = SeparaBlock(728,1024)\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "          SeparaConv(1024,1536),\n",
    "          nn.ReLU(inplace=True),\n",
    "          SeparaConv(1536,2048),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.AdaptiveAvgPool2d(1),\n",
    "          nn.Conv2d(2048,self.n_classes,kernel_size=1,padding=0,stride=1) \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out =self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        return out\n",
    "class MyXcep(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_classes):\n",
    "        super().__init__()\n",
    "        self.entry = Entry()\n",
    "        self.mid = MiddleBlock(728,8)\n",
    "        self.ext = ExitBlock(n_classes)\n",
    "    def forward(self,x):\n",
    "        out = self.entry(x)\n",
    "        out = self.mid(out)\n",
    "        out = self.ext(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    \n",
    "    def __init__(self,ratio,in_channels):\n",
    "        super().__init__()\n",
    "        self.layers=nn.ModuleList()\n",
    "        for i in ratio:\n",
    "            self.layers.append(nn.Conv2d(in_channels,in_channels//len(ratio),kernel_size=3,stride=1,padding=i,dilation=i))\n",
    "        self.layers.append(nn.Conv2d(in_channels,,1))\n",
    "    def forward(self,x):\n",
    "        out =[]\n",
    "        for layer in self.layers:\n",
    "            temp=layer(x)\n",
    "            out.append(temp)\n",
    "        return torch.cat(out,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52cc459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deeplab(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a7c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
